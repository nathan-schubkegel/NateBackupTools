import sys
import os
import ntpath
import hashlib
import re
import datetime
import argparse

# see https://docs.python.org/dev/library/argparse.html
argParser = argparse.ArgumentParser(description='Examines the output of \'hashFolderContents.py\' and reports duplicate files (those with the same MD5 hash), writing them to an output file. The output file looks like:\n<sizeText> [fileName, ...]\n    <path>\n    <path>\n    ....')
argParser.add_argument('hashFilePath', help='The path to the input file (which was generated by \'hashFolderContents.py\'')
argParser.add_argument('outFilePath', help='The path to the output file to populate with detected duplicates')
args = argParser.parse_args()

def human_readable_size(num, suffix='B'):
    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:
        if abs(num) < 1024.0:
            return "%3.1f%s%s" % (num, unit, suffix)
        num /= 1024.0
    return "%.1f%s%s" % (num, 'Yi', suffix)

def splitFilePath(filePath):
    # use ntpath rather than os.path because it treats both kinds 
    # of slashes as path separators

    # drive indicators are annoying, so handle them separately
    (drive, remainingPath) = ntpath.splitdrive(filePath)
   
    # os.path.split is weird about leading slashes, so capture them separately
    leadingSlashes = ""
    poo = re.search("^([\\/]+)(.*)$", remainingPath)
    if poo:
        leadingSlashes = poo.group(1)
        remainingPath = poo.group(2)

    # split dirs and filename
    reversePathParts = []
    while True:
        (base, name) = ntpath.split(remainingPath)
        if base == "":
            if name == "":
                raise ValueError("the file path was empty - not supported")
            else:
                reversePathParts.append(name)
                break
        else:
            if name != "":
                reversePathParts.append(name)
                remainingPath = base
            else:
                raise ValueError("file path ends in a slash - not supported")

    return (drive, leadingSlashes, reversePathParts)

class MemFile:
    def __init__(self, name, fileHash, fileSize, parentDir):
        self.name = name
        self.hash = fileHash
        self.size = fileSize
        self.dir = parentDir

    def getPath(self):
        return self.dir.getPath() + '/' + self.name

class MemDirectory:
    def __init__(self, name, parentDir):
        self.files = {}
        self.dirs = {}
        self.name = name
        self.dir = parentDir

    def getPath(self):
        if self.dir == None:
            return self.name
        return self.dir.getPath() + '/' + self.name

    def add(self, fileHash, fileSize, reversePathParts):
        name = reversePathParts[-1]
        remainingParts = reversePathParts[:-1]
        if len(reversePathParts) == 1:
            newFile = MemFile(name, fileHash, fileSize, self)
            self.files[name] = newFile
            return newFile
        else:
            if name in self.dirs:
                myDir = self.dirs[name]
            else:
                myDir = MemDirectory(name, self)
                self.dirs[name] = myDir
            return myDir.add(fileHash, fileSize, remainingParts)

deduplicatedStrings = {}
def deduplicate(parts):
    i = 0
    count = len(parts)
    while i < count:
        part = parts[i]
        if part in deduplicatedStrings:
            parts[i] = deduplicatedStrings[part]
        else:
            deduplicatedStrings[part] = part
        i = i + 1

inFile = open(args.hashFilePath, "rU") # TODO: what does "rU" mean again?
outFile = open(args.outFilePath, "w")
filesByHash = {}
memRootDir = MemDirectory("", None)
timeOfLastUpdate = datetime.datetime.now()
lineNumber = 0

for line in inFile:
    # report progress once in a while
    lineNumber += 1
    if lineNumber % 500 == 0 and (datetime.datetime.now() - timeOfLastUpdate) > datetime.timedelta(microseconds=500000):
        sys.stdout.write("\rProcessing line " + str(lineNumber) + "...")
        sys.stdout.flush()
        timeOfLastUpdate = datetime.datetime.now()

    # parse line data
    (fileHash, fileSize_forHumes, fileSize, filePath) = line.split(None, 3)
    fileSize = long(fileSize.rstrip("b"))
    (drive, leadingSlashes, reversePathParts) = splitFilePath(filePath)
    deduplicate(reversePathParts)

    # build an in-memory tree of the filesystem
    newFile = memRootDir.add(fileHash, fileSize, reversePathParts)

    # associate all files by hash
    if newFile.hash in filesByHash:
        likeFiles = filesByHash[newFile.hash]
    else:
        likeFiles = []
        filesByHash[newFile.hash] = likeFiles
    likeFiles.append(newFile)

sys.stdout.write("\n")
sys.stdout.flush()

# enumerate all duplicate file piles, sorted first by size then path
print 'Sorting duplicate file data...'
filePiles = [filesByHash[x] for x in filesByHash if len(filesByHash[x]) > 1]
filePiles.sort(key=lambda x: 
        (
            x[0].size, 
            sorted(x, key=lambda y: y.getPath())[0].getPath()
        ), reverse=True)

print 'Writing duplicate file data...'
fileNumber = 0
for filePile in filePiles:
    if fileNumber % 500 == 0 and (datetime.datetime.now() - timeOfLastUpdate) > datetime.timedelta(microseconds=500000):
        sys.stdout.write("\rfile " + str(fileNumber) + "...")
        sys.stdout.flush()
        timeOfLastUpdate = datetime.datetime.now()

    # sanity check for hash collisions
    if not all(x.size == filePile[0].size for x in filePile):
        raise ValueError('Files with same hash had different size!')
  
    # write file size and all unique file names 
    fileSize_forHumes = human_readable_size(filePile[0].size)
    orderedFileNames = sorted(set([x.name for x in filePile]))
    line = fileSize_forHumes + " " + ", ".join(orderedFileNames)
    outFile.write(line + "\n")
    #print line
    fileNumber += 1

    # write a line for each found file path
    for f in sorted(filePile, key=lambda x: x.getPath()):
        line = "    " + f.getPath()
        outFile.write(line + "\n")
        #print line

print 'done'
