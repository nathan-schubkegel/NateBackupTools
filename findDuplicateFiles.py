import sys
import os
import ntpath
import re
import datetime
import argparse
import nateBackupToolsCommon as common
import csv

# see https://docs.python.org/dev/library/argparse.html
argParser = argparse.ArgumentParser(description=
"""Examines the output of \'hashFolderContents.py\' and reports duplicate
files (those with the same MD5 hash), writing them to an output file.
The output file format is csv with space as the delimiter. A row whose
first field is "file" identifies a unique file. The following fields are:
<sizeHumanReadableText> [filename, ...]. 
All rows after that whose first field is "duplicate" identify the paths
of all found matching files. The following field is: <path>.
Empty rows are added for human readability.""")

argParser.add_argument('hashFilePath', 
  help='The path to the input file (which was generated by \'hashFolderContents.py\'')

argParser.add_argument('outFilePath', 
  help='The path to the output file to populate with detected duplicates')

args = argParser.parse_args()

memRootDir = common.MemDirectory(name="", parentDir=None)
filesByHash = {}
fileNumber = 0
timeOfLastUpdate = datetime.datetime.now()
with open(args.hashFilePath, "rb") as inFile:
  csvIn = csv.reader(inFile, delimiter=' ', strict=True)
  for row in csvIn:
    # report progress once in a while
    fileNumber += 1
    if ( \
        fileNumber % 500 == 0 and \
        (datetime.datetime.now() - timeOfLastUpdate) > datetime.timedelta(microseconds=500000)
       ) or (fileNumber == 1):
      sys.stdout.write("\rReading record " + str(fileNumber) + "...")
      sys.stdout.flush()
      timeOfLastUpdate = datetime.datetime.now()

    # parse line data
    (fileHash, fileSize, fileSizeForHumes, filePath) = [row[0], row[1], row[2], row[3]]
    fileSize = long(fileSize)
    (drive, leadingSlashes, reversePathParts) = common.splitFilePath(filePath)
    common.deduplicate(reversePathParts)

    # build an in-memory tree of the filesystem
    newFile = memRootDir.add(fileHash, fileSize, reversePathParts)

    # associate all files by hash
    if newFile.hash in filesByHash:
      likeFiles = filesByHash[newFile.hash]
    else:
      likeFiles = []
      filesByHash[newFile.hash] = likeFiles
    likeFiles.append(newFile)

sys.stdout.write("\rReading record " + str(fileNumber) + "...")
sys.stdout.write("\n")
sys.stdout.flush()

class FilePile:
  def __init__(self, hash, files):
    self.hash = hash
    self.files = files
    self.firstFileSize = files[0].size
    self.firstFilePath = files[0].getPath()

# enumerate all duplicate file piles, sorted first by size then path
piles = []
pileNumber = 0
timeOfLastUpdate = datetime.datetime.now()
for hash in filesByHash:
  files = filesByHash[hash]

  # report progress once in a while
  pileNumber += 1
  if ( \
       pileNumber % 500 == 0 and \
       (datetime.datetime.now() - timeOfLastUpdate) > datetime.timedelta(microseconds=500000) \
     ) or (pileNumber == len(filesByHash)) \
     or (pileNumber == 1):
    sys.stdout.write("\rProcessing unique record " + str(pileNumber) + "...")
    sys.stdout.flush()
    timeOfLastUpdate = datetime.datetime.now()

  if len(files) > 1:
    files.sort(key=lambda y: y.getPath())
    pile = FilePile(hash, files)
    piles.append(pile)

sys.stdout.write("\n")
sys.stdout.flush()

sys.stdout.write("Sorting duplicate file data...\n")
sys.stdout.flush()
piles.sort(key=lambda x: (x.firstFileSize, x.firstFilePath), reverse=True)

pileNumber = 0
timeOfLastUpdate = datetime.datetime.now()
with open(args.outFilePath, "wb") as outFile:
  csvOut = csv.writer(outFile, delimiter=' ', strict=True)
  for filePile in piles:
    pileNumber += 1
    if ( \
        pileNumber % 500 == 0 and \
        (datetime.datetime.now() - timeOfLastUpdate) > datetime.timedelta(microseconds=500000)
       ) or (pileNumber == len(piles)) \
       or (pileNumber == 1):
      sys.stdout.write("\rWriting duplicate file data " + str(pileNumber) + "...")
      sys.stdout.flush()
      timeOfLastUpdate = datetime.datetime.now()

    # sanity check for hash collisions
    if not all(x.size == filePile.firstFileSize for x in filePile.files):
      raise ValueError('Files with same hash had different size!')
  
    # write a row with file size and all unique file names 
    fileSizeForHumes = common.getHumanReadableSize(filePile.firstFileSize)
    orderedFileNames = sorted(set([x.name for x in filePile.files]))
    csvOut.writerow(["file", fileSizeForHumes] + orderedFileNames)

    # write a row with each found file path
    for f in filePile.files:
      csvOut.writerow(["duplicate", f.getPath()])

    # write a blank row for readability
    csvOut.writerow([])

sys.stdout.write("\n")
sys.stdout.flush()
print 'done'
