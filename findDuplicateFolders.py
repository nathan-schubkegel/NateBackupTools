import sys
import argparse
import nateBackupToolsCommon as common
import csv

# see https://docs.python.org/dev/library/argparse.html
argParser = argparse.ArgumentParser(description=
"""Examines the output of \'hashFolderContents.py\' and reports duplicate
folders (those with mostly same file contents), writing them to an output file.

The output file format is csv with space as the delimiter. 

A series of rows whose first field is "parent" identifies folders
that contain mostly-matching contents. The remaining fields in the rows are:
<sizeHumanReadableText> <path>.

A series of rows whose first field is "folder" or "file" identifies the child
folders and files of all the matched "parent" folders. The remaining fields
in the rows are: <index> <sizeHumanReadableText> <similarity> <path>.
index indicates which other "folder" or "file" rows are correlated.
similarity is one of "same", "like", or "diff".

Empty rows are added between duplicate folders for human readability.""")

argParser.add_argument('hashFilePath', 
  help='The path to the input file (which was generated by \'hashFolderContents.py\'')

argParser.add_argument('outFilePath', 
  help='The path to the output file to populate with detected duplicates')

args = argParser.parse_args()

memRootDir = common.MemDirectory(name="", parentDir=None)
filesByHash = {}
progress = common.ProgressPrinter("\rReading record {0}...")
with open(args.hashFilePath, "rb") as inFile:
  csvIn = csv.reader(inFile, delimiter=' ', strict=True)
  for row in csvIn:
    # report progress once in a while
    progress.report()

    # parse line data
    (fileHash, fileSize, fileSizeForHumes, filePath) = [row[0], row[1], row[2], row[3]]
    fileSize = long(fileSize)
    (drive, leadingSlashes, reversePathParts) = common.splitFilePath(filePath)
    common.deduplicate(reversePathParts)

    # build an in-memory tree of the filesystem
    newFile = memRootDir.add(fileHash, fileSize, reversePathParts)

    # associate all files by hash
    if newFile.hash in filesByHash:
      likeFiles = filesByHash[newFile.hash]
    else:
      likeFiles = []
      filesByHash[newFile.hash] = likeFiles
    likeFiles.append(newFile)

    # sanity check for hash collisions
    if not likeFiles[0].size == newFile.size:
      raise ValueError('Files with same hash had different size!')
  
progress.reportDone()

stuffToIgnore2 = {}
likeDirsByDir = {}
progress2 = common.ProgressPrinter("\rProcessing folder {0}...")

def lookForDuplicateFolders(memDir):
  # recurse into child directories first
  for dirName in memDir.dirs:
    childDir = memDir.dirs[dirName]
    lookForDuplicateFolders(childDir)

  # report progress once in a while
  progress2.report()

  # get all other directories containing files like any of my files
  likeDirs = {}
  for fileName in memDir.files:
    file = memDir.files[fileName]
    likeFiles = filesByHash[file.hash]
    for otherFile in likeFiles:
      if otherFile.dir != memDir and otherFile.dir is not None:
        likeDirs[otherFile.dir] = otherFile.dir

  # eliminate all other directories that have already been associated with some other folder
  for likeDir in list(likeDirs):
    if stuffToIgnore2.get(likeDir) is not None:
      likeDirs.pop(likeDir)

  # only retain directories if at least 50% or more of both our files match
  for likeDir in list(likeDirs):
    likeFileCount = 0
    filesToIgnore = {}
    for fileName in likeDir.files:
      otherFile = likeDir.files[fileName]
      likeFiles = filesByHash[otherFile.hash]
      for likeFile in likeFiles:
        if likeFile.dir == memDir \
            and otherFile not in filesToIgnore \
            and likeFile not in filesToIgnore:
          likeFileCount += 1
          filesToIgnore[likeFile] = likeFile
          filesToIgnore[otherFile] = otherFile
          break
    if (likeFileCount < len(memDir.files) / 2) or (likeFileCount < len(likeDir.files) / 2):
      likeDirs.pop(likeDir, None)

  # add self to results so some later logic is simpler
  likeDirs[memDir] = memDir

  # save results
  likeDirsByDir[memDir] = likeDirs

  # do not associate these directories with other directories later
  for likeDir in likeDirs:
    stuffToIgnore2[likeDir] = likeDir

# enumerate all folders looking for similar ones
lookForDuplicateFolders(memRootDir)
progress2.reportDone()

# enumerate all duplicate folder piles, sorted first by size then path
print "Accumulating similar folders..."
duplicateLikeDirs = [ \
    sorted(likeDirsByDir[x], key=lambda d: (d.size, d.getPath())) \
    for x in likeDirsByDir if len(likeDirsByDir[x]) > 1]

print "Sorting similar folders..."
duplicateLikeDirs.sort(key=lambda d: (d[0].size, d[0].getPath()))

progress = common.ProgressPrinter("\rWriting duplicate folder data {0}...")
with open(args.outFilePath, "wb") as outFile:
  csvOut = csv.writer(outFile, delimiter=' ', strict=True)
  dirsToIgnore = {}
  for likeDirs in duplicateLikeDirs:

    # only show directories similar to others once
    for likeDir in list(likeDirs):
      if likeDir in dirsToIgnore:
        likeDirs.remove(likeDir)

    if len(likeDirs) <= 1:
      continue

    for likeDir in likeDirs:
      dirsToIgnore[likeDir] = likeDir

    # report progress once in a while
    progress.report()

    # write rows indicating the parent folder sizes and paths
    parentIndex = 0
    orderedLikeDirs = sorted(likeDirs, key=lambda x: x.getPath())
    for likeDir in orderedLikeDirs:
      parentIndex += 1
      fileSizeForHumes = common.getHumanReadableSize(likeDir.size)
      csvOut.writerow(["parent", fileSizeForHumes, likeDir.getPath()])

    # figure out which files are alike again TODO: it is inefficient to do this twice, right?
    filesByHash = {}
    for likeDir in likeDirs:
      for someFileName in likeDir.files:
        someFile = likeDir.files[someFileName]
        if someFile.hash in filesByHash:
          filesByHash[someFile.hash].append(someFile)
        else:
          filesByHash[someFile.hash] = [someFile]

    for pile in filesByHash.itervalues():
      pile.sort(key=lambda x: x.name)
   
    # associate similar files from parent directories
    subPiles = []
    while len(filesByHash) > 0:
      pileKey = filesByHash.iterkeys().next()
      pile = filesByHash[pileKey]

      if len(pile) == 1:
        subPiles.append({pile[0].dir: pile[0]})
        filesByHash.pop(pileKey)
      else:
        # associate a file from each parent directory
        # (there may be multiple files from one parent with the same hash... so just add 1 from each parent)
        subPile = {}
        subPiles.append(subPile)
        for likeDir in likeDirs:
          likeFile = next((x for x in pile if x.dir == likeDir), None)
          if likeFile is not None:
            subPile[likeDir] = likeFile
            pile.remove(likeFile)
        # remove
        if len(pile) == 0:
          filesByHash.pop(pileKey)

    # sort associations by number of similar files, then by file size, then by some file name
    subPiles.sort(key=lambda subPile: \
        ( \
          -len(subPile), \
          subPile.itervalues().next().size, \
          subPile.itervalues().next().name \
        ))

    # write rows indicating the associations
    fileIndex = 0
    for subPile in subPiles:
      fileIndex += 1
      for likeDir in orderedLikeDirs:
        likeFile = subPile.get(likeDir)
        if likeFile is not None:
          fileSizeForHumes = common.getHumanReadableSize(likeFile.size)
          similarity = "same" if len(subPile) > 1 else "diff"
          csvOut.writerow(["file", fileIndex, fileSizeForHumes, similarity, likeFile.getPath()])

    # write a blank row for readability
    csvOut.writerow([])

progress.reportDone()
print 'done'
